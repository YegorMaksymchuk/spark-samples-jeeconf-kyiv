# spark://127.0.0.1:7077 or local
# local[n] is a special value that runs Spark on n threads on the local machine,
# without connecting to a cluster.
spark.master=local[2]

spark.application-name=Spark Presentation

spark.distributed-libraries=file:///Users/tmatyashovsky/Workspace/spark-samples-jeeconf-kyiv/spark-distributed-library/target/spark-distributed-library-1.0-SNAPSHOT-uber.jar

# The maximum amount of CPU cores to request for the application from across the cluster (not from each machine).
spark.cores.max=2

# Amount of memory to assign to each executor process
spark.executor.memory=2g

# The largest number of partitions in a parent RDD during distributed shuffle operations.
# For local mode should be equal to number of cores on the local machine.
spark.default.parallelism=4

# Serializer: org.apache.spark.serializer.JavaSerializer (default) or org.apache.spark.serializer.KryoSerializer
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false

# The number of partitions to use when shuffling data for joins or aggregations.
spark.sql.shuffle.partitions=5

# 2 options:
# - localDataFilesLocation
# - hdfsDataFilesLocation
dataFilesLocation=localDataFilesLocation

spark.path-to-csv-data=/Users/tmatyashovsky/Work/Spark-Presentation-JEEConf-Kyiv/csv_test
spark.path-to-parquet-data=/Users/tmatyashovsky/Work/Spark-Presentation-JEEConf-Kyiv/parquet

# 4 options:
# - reflectionDataReader
# - programmaticDataReader
# - parquetDataReader
# - postgresDataReader
dataFrameDataFilesReader=reflectionDataReader